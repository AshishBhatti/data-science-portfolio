{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9c3a12-b575-4a52-9dc0-6e96f17b4759",
   "metadata": {},
   "source": [
    "# Coding with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27424eac-2457-46db-bd2c-c942a1e5a4f7",
   "metadata": {},
   "source": [
    "- **Convolutions**: Use filters to extract information from images\n",
    "- **Pooling**: Can reduce and compress the images without losing the vital information that was extracted by the filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7ebb7-7a57-43b0-8524-8b86c7d247cd",
   "metadata": {},
   "source": [
    "In previous labs, to classify fashion images and handwriting, we defined a model architecture like this. We used primarily dense layers for densely connected neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501feba8-1ebe-4bf0-b689-7a6411a98181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7845ac9-a794-402f-8159-58a7cbbd2c58",
   "metadata": {},
   "source": [
    "Now to use convolutions and pooling we use the following layers on top of the previous architecture.\n",
    "- For Convolution : Conv2D\n",
    "- For Pooling: MaxPooling2D\n",
    "\n",
    "The new architecture would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358e1d1f-d326-47aa-a17f-3573c1433712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', \n",
    "                        input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c1f92-c211-411c-8361-9aff4a49e747",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Convolution layer takes a number of parameter\n",
    "```\n",
    "Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1))\n",
    "```\n",
    "- 64 is the number of filters for this layer. These filters will be randomly initialized. The best filters to match the pictures to their labels will be learned over time.\n",
    "- (3,3) is the size of the filter. _(kernel matrix size)_\n",
    "- (28,28,1) is the input shape of the images being fed in. 28x28 pixels with one byte color-depth.\n",
    "  \n",
    "2. Similarly pooling is done with the following layer\n",
    "```\n",
    "MaxPooling2D(2, 2)\n",
    "```\n",
    "- 2 by 2 is the size of the chunks to pool.\n",
    "- There is also MinPooling, AveragePooling, but here we only focus on MaxPooling.\n",
    "\n",
    "The group of these 2 layers are stacked on top of each other.\n",
    "For example above, the results of the 64 filters from the top layer will each be pooled, and then their results will each be filtered 64 times, and they ofcourse will get pooled again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397df5c2-5725-4848-b5eb-5cb1e16ea0af",
   "metadata": {},
   "source": [
    "To get a sense of how the data changes when it goes through the network we use ```model.summary()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc723e31-9a26-457d-aff8-e493c5b36ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243786 (952.29 KB)\n",
      "Trainable params: 243786 (952.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03796bd-5af9-4be2-baaa-14d365f4607a",
   "metadata": {},
   "source": [
    "Lets unpack this:\n",
    "1. ```(None, 26, 26, 64)``` \n",
    "   - The input image size is 28x28, so why is the output size 26x26.\n",
    "   We saw previously that, (3x3) filters operate on the middle value and require neighbours around the pixel. Hence, the convolution operation begins at the second pixel of second row. Thus, the first row, last row, fist column and last column are not included in the convoluted image. \n",
    "   - 64 is the number of filters. Hence the output has 64 images of 26x26 dimensions each.\n",
    "   - 640 params, because each filter learns 9 weights and 1 bias, so 10 params per filter.\n",
    "2. ```(None, 13, 13, 64)```\n",
    "   - Our pooling reduced the dimensionality by half on each axis, so 26x26 becomes 13x13.\n",
    "   - No parameters are learned in this layer.\n",
    "3. ```(None, 11, 11, 64)```\n",
    "   - 3x3 filter reduces 13x13 to 11x11, by removing a pizel border like before.\n",
    "   - How are the params 36928?\n",
    "4. ```(None, 5, 5, 64)```\n",
    "   - MaxPooling again halves the dimensions, rounding down. So we end up with 5 by 5 images.\n",
    "   - At this point we have 64 filters and images are 5by5. Multiplying it all we get 1600.\n",
    "5. ```(None, 1600)```\n",
    "   - Flatten takes these 64 images of 5by5, and make a 1D array.\n",
    "\n",
    "These set of 1600 values are classified using a Dense network like before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
