{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd94d32d-e186-4c51-9577-dba2059d3b39",
   "metadata": {},
   "source": [
    "# Lab5: Classifying real-world images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b9bae-9aac-4da1-bd0c-aa6a35a8cd46",
   "metadata": {},
   "source": [
    "In last exercise we built a CNN to classify MNIST images. \n",
    "- We also used a callback to cancel training if a certain accuracy is reached.\n",
    "- We reshaped our training and test images in order to pass them to convolution layer.\n",
    "\n",
    "We improved the performance of classification by using convolutions which spotted features in the image. The rest of the network matched those features to the labels instead of using raw pixels and hoping for the best.\n",
    "\n",
    "This technique in theory should work for images that are more complex than the fashion-mnist and mnist dataset. Here we will see if we can apply the same technique to other images. \\\n",
    "_(Images in which subject is not centered and facing in the same direction)_\n",
    "\n",
    "For this we will use **horses_and_humans dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d33ce1a-2c4d-484f-8a2c-14d36145ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse01-0.png', 'horse01-1.png', 'horse01-2.png', 'horse01-3.png', 'horse01-4.png', 'horse01-5.png', 'horse01-6.png', 'horse01-7.png', 'horse01-8.png', 'horse01-9.png']\n",
      "['human01-00.png', 'human01-01.png', 'human01-02.png', 'human01-03.png', 'human01-04.png', 'human01-05.png', 'human01-06.png', 'human01-07.png', 'human01-08.png', 'human01-09.png']\n",
      "total training horse images:  500\n",
      "total training human images:  527\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset_path = \"D:\\\\local_ws\\\\datasets\\\\horse-or-human\\\\\"\n",
    "\n",
    "# specify directories:\n",
    "train_horse_dir = os.path.join(dataset_path + 'train\\\\horses')\n",
    "train_human_dir = os.path.join(dataset_path + 'train\\\\humans')\n",
    "validation_horse_dir = os.path.join(dataset_path + 'validation\\\\horses')\n",
    "validation_human_dir = os.path.join(dataset_path + 'validation\\\\humans')\n",
    "\n",
    "# check how the filenames look like in these directories\n",
    "train_horse_names = os.listdir(train_horse_dir); print(train_horse_names[:10])\n",
    "train_human_names = os.listdir(train_human_dir); print(train_human_names[:10])\n",
    "\n",
    "# the total number of horse and human images in the directories\n",
    "print('total training horse images: ', len(train_horse_names))\n",
    "print('total training human images: ', len(train_human_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a48a0eff-3ef5-4ee9-b774-2d0c4ac6e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the pictures\n",
    "\n",
    "# First configuring matplotlib parameters\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Params of our graph, we will output these images in 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2680576-aeae-4145-b17a-f14c24be5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a batch of 8 horse and 8 human pictures.\n",
    "# Rerun this cell to see a fresh batch each time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c1084-13c2-4fba-a283-6a9f1296e4b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "As we will not be loading the dataset through inbuilt function, we will need an easy way to load the dataset.\n",
    "\n",
    "**Tensorflow supports the use of sub-directories.**\n",
    "- If I have a master directory which contains two sub-directories called Training and Validation.\n",
    "- In each of these sub-directories, I have two folders with name horses and humans, which contain the images which we want to use.\n",
    "\n",
    "With just this, I now have a fully labelled dataset for training and testing. Because with tensorflow, I can pass these directories to something called a generator and it will auto-label these images based on directory name. This labelling is achieved using ```ImageDataGenerator``` in the ```keras``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b68e6-39bb-471f-a89d-a50a9e0e1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ade6d5-5993-45d0-bb81-bf51c3cfa6b7",
   "metadata": {},
   "source": [
    "We can use this generator to apply transformations on the image. In this example we just apply normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84727d97-8c4e-45e2-b6f9-a01ef5ebb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=128,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3836ee47-48f2-44e1-863c-51d4c3274fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for simple CNN that can classify horses and images\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # 9 layers, 3 (convolution, maxpool) pairs, 1 flatten, 2 dense\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu',\n",
    "                           input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90860419-561f-4f2a-b772-d35e811bc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 149, 149, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 73, 73, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 35, 35, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 78400)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               40141312  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40165409 (153.22 MB)\n",
      "Trainable params: 40165409 (153.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414831e-2847-433b-8801-7f31081b2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd4de1-ae75-4eec-9422-fafa671e7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=8,\n",
    "    validation_steps=8.\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59796544-856c-40dd-8764-8e7dcb1958f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6aada0-8d9e-449e-8418-6d8fb1f88823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2279b-5b85-4287-988a-273aa76f015f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac33c55-3d8c-4148-aada-f51e8d6dffd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
